---
title: Reproducible research with the OpenAI API
output: html_document
---

## Reproducible research with the OpenAI API

Quantitative and mixed methods research in the social sciences -- or any 
interdisciplinary research -- usually entails a whole lot of grunt work. 
ChatGPT is a decent solution for a subset of this work, when results can be 
easily verified and/or the decisions are not critical because they just prepare
data for later stages. In some cases, ChatGPT can even be a superior 
alternative. Imagine you want to trace hundreds of actors across multiple datasets.
You would expect to find many variations of spelling or specificity across these
datasets. If you can load all entities across all datasets into memory at one 
time, you can expect ChatGPT to do much better than any human coder ever would.
However, some challenges remain. Three were most salient to me. (1) Documenting
and iterating over the procedure -- this can be resolved by calling the API 
from code instead of using the browser-based chat window. (2) Models provide a 
text response and the formatting might vary between runs -- when using OpenAI
this can be alleviated by specifically requesting a json output. (3) The 
output of large language models is random -- when using OpenAI, this issue can 
be (partially?) alleviated by providing a seed. 

Some housekeeping.

```{r library, message = FALSE}
library(tidyverse)
library(jsonlite)

# Change default value for toJSON auto_unbox argument since we always need this.
formals(toJSON)$auto_unbox <- T
```

### 1. Document your procedures by using an API

To use the API of any large language model, we first need to create an API key.
For OpenAI, this requires a premium subscription at $20 per month. Next, 
at platform.openai.com we can generate our API key. I created a variable, 
jb_chatgpt_key, in my .RProfile file to always have this key handy when I need
it. One way to call the OpenAI API is to use their official Python package.
To integrate OpenAI into our datascience workflows, we could use the unofficial
R openai package, but all does for us is to call the OpenAI API and it lags 
behind the official package, so I prefer to call the OpenAI API directly. 
We need to provide three things.

1. The API endpoint (i.e., a URL). Some API endpoints are interoperable, that
    is, we just need to change the URL to receive a response to our query from
    a different provider.

```{r API}
chatgpt_url <- "https://api.openai.com/v1/chat/completions"
```

2. In the header, we provide our API key. Since this information is not 
    specific to our request, we can reuse the header.
    
```{r header}
headers <- httr::add_headers(c(
  `Content-Type` = "application/json",
  Authorization = paste("Bearer", jb_chatgpt_key)
  ))
```
    
3. In the body, we include the content of our request. We can also include our
    settings here, such as the model to use. Yes, the nested lists are 
    awkward and easy to get wrong.

```{r body}
prompt <- "Create a list of five cities and their population counts."
body <- toJSON(
  list(
    model = "gpt-4o",
    messages = list(
# We're required to nest here because we might want to include a role and 
# content.
      list(role = "user", content = prompt)
    )))
```

Now we can make the call to the API:

```{r call}
message <- httr::POST(url = chatgpt_url,
                      config = headers,
                      body = body)
```

The response includes a few information that we may need for debugging:

```{r overview}
message_content <- httr::content(message)
glimpse(message_content)
```

If our call is successful, we only need one piece of information from this --
the message. Again, extracting it is a bit awkward because of the nested 
nature of the API response.

```{r message}
cat(message_content$choices[[1]]$message$content)
```

Let's run the same query again just to see the difference.

```{r again}
httr::POST(url = chatgpt_url,
           config = headers,
           body = body) %>%
  httr::content() %>%
  {cat(.$choices[[1]]$message$content)}
```

Big difference in formatting, which makes the output quite unpredictable for 
parsing.

### 2. Requesting a consistent and parsable JSON response

See: https://platform.openai.com/docs/guides/text-generation/json-mode

Fortunately for us, OpenAI allows an option to explicitly request a JSON 
formatted response. This mode is only supported by gpt-4o, gpt-4-turbo, and 
gpt-3.5-turbo. One model that does not support JSON mode is gpt-4. Fortunately,
the performance of gpt-4-turbo is almost on par with gpt-4. I could not find any
notable differences between the two.

```{r json_body}
# ChatGPT requires us to insert the term json into the query, too.
json_prompt <-
  "Create a list of five cities and their population counts in json."
json_body <- toJSON(
  list(
    model = "gpt-4o",
    response_format = list(type = "json_object"),
    messages = list(
      list(role = "user", content = json_prompt)
    )))
httr::POST(url = chatgpt_url, config = headers, body = json_body) %>%
  {httr::content(.)} %>%
  {fromJSON(.$choices[[1]]$message$content)[[1]]}
```

Now we receive a json object that we can reliably parse into a dataframe by 
just calling ```fromJSON``` from the jsonlite package.

### 3. Controlling randomness by setting a seed

See: https://platform.openai.com/docs/api-reference/chat/create

Next, we want to control the randomness of the process. We receive our 
response in a consistent format now, but the content of the responses still
varies. That is, when a colleague later reruns their query, they will receive a
different response than you. OpenAI's seed feature is still in Beta, and it's 
not documented well. On a side note, I tried the same prompt but with 
imaginary cities first, and I did not receive a consistent output. So there are
still limits to the seed parameter.

```{r function}
seeded_request <- function(model_, seed_){
  seeded_body <- toJSON(
    list(
      model = model_,
      response_format = list(type = "json_object"),
      seed = seed_,
      messages = list(
        list(role = "user", content = json_prompt)
      )))
  httr::POST(url = chatgpt_url, config = headers, body = seeded_body) %>%
    {httr::content(.)} %>%
    {fromJSON(.$choices[[1]]$message$content)[[1]]}
}
```

Let's run a few queries and see which models support the parameter.

#### gpt-4o

```{r gpt-4o}
seeded_request("gpt-4o", 13)
seeded_request("gpt-4o", 13)
seeded_request("gpt-4o", 999)
```

This worked. 

#### gpt-4-turbo

```{r gpt-4-turbo}
seeded_request("gpt-4-turbo", 13)
seeded_request("gpt-4-turbo", 13)
seeded_request("gpt-4-turbo", 999)
```

This worked to some degree?

#### gpt-4

```{r gpt-4, error = T}
seeded_request("gpt-4", 13)
seeded_request("gpt-4", 13)
```

This does not work at all.

#### gpt-3.5

```{r gpt-3.5}
seeded_request("gpt-3.5-turbo", 13)
seeded_request("gpt-3.5-turbo", 13)
seeded_request("gpt-3.5-turbo", 999)
```

This works again with some minor variation. Notably, the response starts out
consistent but then starts to vary. So maybe that's the limitation of the 
seed parameter. Accordingly, I would expect that the longer the response, the
greater the variation. So for very long responses (like we usually submit) 
the parameter may limit variation, but will not remove it.




